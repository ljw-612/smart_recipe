{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from PyPDF2 import PdfReader\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "from datasets import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option(\n",
    "    \"display.max_colwidth\", None\n",
    ")  # This will be helpful when visualizing retriever outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "def extract_text(pdf_files):\n",
    "    \"\"\"\n",
    "    Function to extract the text from a PDF file\n",
    "\n",
    "    Args:\n",
    "        pdf_file (file): The PDF files to extract the text from\n",
    "\n",
    "    Returns:\n",
    "        text (str): The extracted text from the PDF file\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the raw text variable\n",
    "    text = \"\"\n",
    "\n",
    "    # Iterate over the documents\n",
    "    for pdf_file in pdf_files:\n",
    "\n",
    "        # Read the PDF file\n",
    "        pdf_reader = PdfReader(pdf_file)\n",
    "\n",
    "        # Extract the text from the PDF pages and add it to the raw text variable\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text()\n",
    "    \n",
    "    return text\n",
    "\n",
    "raw_text = extract_text(pdf_files=[\"../data/data.pdf\"])\n",
    "\n",
    "RAW_KNOWLEDGE_BASE = [\n",
    "    LangchainDocument(page_content=raw_text, metadata={\"source\":\"food imcompatibility\"})\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "MARKDOWN_SEPARATORS = [\n",
    "    \"\\n#{1,6} \",\n",
    "    \"```\\n\",\n",
    "    \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
    "    \"\\n---+\\n\",\n",
    "    \"\\n___+\\n\",\n",
    "    \"\\n\\n\",\n",
    "    \"\\n\",\n",
    "    \" \",\n",
    "    \"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "EMBEDDING_MODEL_NAME = \"thenlper/gte-small\"\n",
    "\n",
    "def split_documents(\n",
    "    chunk_size: int,\n",
    "    knowledge_base: List[LangchainDocument],\n",
    "    tokenizer_name: Optional[str] = EMBEDDING_MODEL_NAME,\n",
    ") -> List[LangchainDocument]:\n",
    "    \"\"\"\n",
    "    Split documents into chunks of maximum size `chunk_size` tokens and return a list of documents.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size / 10),\n",
    "        add_start_index=True,\n",
    "        strip_whitespace=True,\n",
    "        separators=MARKDOWN_SEPARATORS,\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in knowledge_base:\n",
    "        docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "    # Remove duplicates\n",
    "    unique_texts = {}\n",
    "    docs_processed_unique = []\n",
    "    for doc in docs_processed:\n",
    "        if doc.page_content not in unique_texts:\n",
    "            unique_texts[doc.page_content] = True\n",
    "            docs_processed_unique.append(doc)\n",
    "\n",
    "    return docs_processed_unique\n",
    "\n",
    "docs_processed = split_documents(\n",
    "    512,  # We choose a chunk size adapted to our model\n",
    "    RAW_KNOWLEDGE_BASE,\n",
    "    tokenizer_name=EMBEDDING_MODEL_NAME,\n",
    ")\n",
    "\n",
    "# Let's visualize the chunk sizes we would have in tokens from a common model\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME)\n",
    "# lengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(docs_processed)]\n",
    "# fig = pd.Series(lengths).hist()\n",
    "# plt.title(\"Distribution of document lengths in the knowledge base (in count of tokens)\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    # multi_process=True,\n",
    "    # model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},  # Set `True` for cosine similarity\n",
    ")\n",
    "\n",
    "KNOWLEDGE_VECTOR_DATABASE = FAISS.from_documents(\n",
    "    docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================Top document==================================\n",
      "猪⾁\n",
      "　　 （1）不宜⻝⽤未摘除甲状腺的猪⾁\n",
      "　　 （2）服降压药和降⾎脂药时不宜多⻝\n",
      "　　 （3）禁忌⻝⽤猪油渣\n",
      "　　 （4）⼩⼉不宜多⻝\n",
      "　　 （5）不宜在刚屠后煮⻝\n",
      "　　 （6）未剔除肾上腺和病变的淋巴结时不宜⻝⽤\n",
      "　　 （7）⽼⼈不宜多⻝瘦⾁\n",
      "　　 （8）⻝⽤前不宜⽤热⽔浸泡\n",
      "　　 （9）在烧煮过程中忌加冷⽔\n",
      "　　 （10）不宜多⻝煎炸咸⾁\n",
      "　　 （11）不宜多⻝加硝腌制之猪⾁\n",
      "　　 （12）不宜多⻝午餐⾁\n",
      "　　 （13）不宜多⻝肥⾁\n",
      "　　 （14）忌与鹌鹑同⻝,同⻝令⼈⾯⿊\n",
      "　　 （15）忌与鸽⾁、鲫⻥、虾同⻝,同⻝令⼈滞⽓\n",
      "　　 （16）忌与荞⻨同⻝,同⻝令⼈落⽑发\n",
      "　　 （17）忌与菱⻆、⻩⾖、蕨菜、桔梗、乌梅、百合、巴⾖、⼤⻩、⻩连、苍术、芜荽\n",
      "同⻝\n",
      "　　 （18）忌与⽜⾁、驴⾁（易致腹泻）、⽺肝同⻝。\n",
      "　　 （19）服磺胺类药物时不宜多⻝\n",
      "　　猪肝\n",
      "　　 （1）忌与荞⻨、⻩⾖、⾖腐同⻝,同⻝发痼疾\n",
      "　　 （2）忌与⻥⾁同⻝,否则令⼈伤神　　 （3）忌与雀⾁、⼭鸡、鹌鹑⾁同⻝\n",
      "　　猪⾎\n",
      "　　（1）忌⻩⾖,同⻝令⼈⽓滞\n",
      "　　 （2）忌地⻩、何⾸乌\n",
      "　　⽺⾁\n",
      "　　 （1）不宜多⻝烤⽺⾁串\n",
      "　　 （2）不宜⻝⽤反复剩热或冻藏加温的⽺⾁\n",
      "==================================Metadata==================================\n",
      "{'source': 'food imcompatibility', 'start_index': 7324}\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=\"鲫鱼最好不和什么一起吃？\", k=5)\n",
    "print(\n",
    "    \"\\n==================================Top document==================================\"\n",
    ")\n",
    "print(retrieved_docs[0].page_content)\n",
    "print(\"==================================Metadata==================================\")\n",
    "print(retrieved_docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"http://127.0.0.1:8080/v1\",\n",
    "    api_key=openai_api_key\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='鲫鱼最好不和“海味”一起吃。因为海味中含有鞣酸，如果与含有鞣酸的水果（如苹果）一起食用，可能会引起腹痛、恶心、呕吐等不适。<|eot_id|>', role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs_text = [\n",
    "    doc.page_content for doc in retrieved_docs\n",
    "]\n",
    "context = \"\\nExtracted documents:\\n\"\n",
    "context += \"\".join(\n",
    "    [f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(retrieved_docs_text)]\n",
    ")\n",
    "query = \"鲫鱼最好不和什么一起吃？\"\n",
    "\n",
    "\n",
    "RAG_PROMPT_TEMPLATE = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"Using the information contained in the context,\n",
    "give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Provide the number of the source document when relevant.\n",
    "If the answer cannot be deduced from the context, do not give an answer.\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"Context:\n",
    "{context}\n",
    "---\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: {question}\"\"\".format(context=context, question=query),\n",
    "    },\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"LLaMA_CPP\",\n",
    "    messages=RAG_PROMPT_TEMPLATE\n",
    ")\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'鲫鱼最好不和“海味”一起吃。因为海味中含有鞣酸，如果与含有鞣酸的水果（如苹果）一起食用，可能会引起腹痛、恶心、呕吐等不适。<|eot_id|>'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_590",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
