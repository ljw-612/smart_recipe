{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from dotenv import load_dotenv\n",
    "from PyPDF2 import PdfReader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import sqlite3\n",
    "import pickle\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(pdf_files):\n",
    "    \"\"\"\n",
    "    Function to extract the text from a PDF file\n",
    "\n",
    "    Args:\n",
    "        pdf_file (file): The PDF files to extract the text from\n",
    "\n",
    "    Returns:\n",
    "        text (str): The extracted text from the PDF file\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the raw text variable\n",
    "    text = \"\"\n",
    "\n",
    "    # Iterate over the documents\n",
    "    for pdf_file in pdf_files:\n",
    "\n",
    "        # Read the PDF file\n",
    "        pdf_reader = PdfReader(pdf_file)\n",
    "\n",
    "        # Extract the text from the PDF pages and add it to the raw text variable\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(text):\n",
    "    \"\"\"\n",
    "    Function to get the chunks of text from the raw text\n",
    "\n",
    "    Args:\n",
    "        text (str): The raw text from the PDF file\n",
    "\n",
    "    Returns:\n",
    "        chunks (list): The list of chunks of text\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the text splitter\n",
    "    splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\",  # Split the text by new line\n",
    "        chunk_size=1000,  # Split the text into chunks of 1000 characters\n",
    "        chunk_overlap=200,  # Overlap the chunks by 200 characters\n",
    "        length_function=len,  # Use the length function to get the length of the text\n",
    "    )\n",
    "\n",
    "    # Get the chunks of text\n",
    "    chunks = splitter.split_text(text)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embeddings(client, chunks):\n",
    "    \"\"\"\n",
    "    This function creates embeddings for the chunks of text using the OpenAI API.\n",
    "    \"\"\"\n",
    "\n",
    "    def _make_embedding(client, chunk, model=\"text-embedding-3-small\"):\n",
    "        chunk = chunk.replace(\"\\n\", \" \")\n",
    "        return client.embeddings.create(input=[chunk], model=model).data[0].embedding\n",
    "\n",
    "    embeddings = []\n",
    "    for chunk in chunks:\n",
    "        embedding = _make_embedding(client, chunk)\n",
    "        embeddings.append(embedding)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database(database_name):\n",
    "    \"\"\"\n",
    "    This funciton creates a database to store the embeddings.\n",
    "    Columns: id, text, embedding\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(database_name)\n",
    "    c = conn.cursor()\n",
    "    c.execute(\n",
    "        \"SELECT count(name) FROM sqlite_master WHERE type='table' AND name='embeddings'\"\n",
    "    )\n",
    "    if c.fetchone()[0] == 0:\n",
    "        # If the table doesn't exist, create it\n",
    "        c.execute(\n",
    "            \"\"\"CREATE TABLE embeddings\n",
    "                     (id INTEGER PRIMARY KEY,\n",
    "                     text TEXT,\n",
    "                     embedding BLOB)\"\"\"\n",
    "        )\n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_embedding(database_name, text, embedding):\n",
    "    \"\"\"\n",
    "    This function inserts the text and its embedding into the database.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(database_name)\n",
    "    c = conn.cursor()\n",
    "\n",
    "    embedding_serialized = pickle.dumps(embedding)\n",
    "    c.execute(\n",
    "        \"INSERT INTO embeddings (text, embedding) VALUES (?, ?)\",\n",
    "        (text, embedding_serialized),\n",
    "    )\n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_text(database_name, query_embedding, num_results=5):\n",
    "    \"\"\"\n",
    "    This function performs the\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(database_name)\n",
    "    c = conn.cursor()\n",
    "    c.execute(\"SELECT text, embedding FROM embeddings\")\n",
    "    results = c.fetchall()\n",
    "    conn.close()\n",
    "    results = [(text, pickle.loads(embedding)) for text, embedding in results]\n",
    "\n",
    "    # calculate the cosine similarity\n",
    "    similarities = []\n",
    "    for text, embedding in results:\n",
    "        query_embedding = np.array(query_embedding).reshape(1, -1)\n",
    "        embedding = np.array(embedding).reshape(1, -1)\n",
    "\n",
    "        # calculate the cosine similarity\n",
    "        similarity = cosine_similarity(query_embedding, embedding)[0][0]\n",
    "        similarities.append((text, embedding, similarity))\n",
    "    similarities.sort(key=lambda x: x[2], reverse=True)\n",
    "    # get the top 5 similar texts\n",
    "    return similarities[:num_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(\n",
    "    client,\n",
    "    system_content=\"\",\n",
    "    assistant_content=\"\",\n",
    "    user_content=\"\",\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_content},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_content},\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "        ],\n",
    "        # stream=True,\n",
    "    )\n",
    "    return chat_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_db_exists(database_name):\n",
    "    conn = sqlite3.connect(database_name)\n",
    "    c = conn.cursor()\n",
    "    c.execute(\n",
    "        \"SELECT count(name) FROM sqlite_master WHERE type='table' AND name='embeddings'\"\n",
    "    )\n",
    "    if c.fetchone()[0] == 0:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database already exists.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "raw_text = extract_text(pdf_files=[\"../data/data.pdf\"])\n",
    "\n",
    "chunks = get_chunks(text=raw_text)\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "database_name = \"smartRecipe.db\"\n",
    "\n",
    "if not check_db_exists(database_name):\n",
    "    print(\"Embedding database does not exist. Creating one...\")\n",
    "\n",
    "    embeddings = make_embeddings(client, chunks)\n",
    "\n",
    "    create_database(database_name)\n",
    "\n",
    "    for chunk, embedding in zip(chunks, embeddings):\n",
    "        insert_embedding(database_name, chunk, embedding)\n",
    "\n",
    "    print(\"Database created.\")\n",
    "else:\n",
    "    print(\"Database already exists.\")\n",
    "\n",
    "prompt = \"鲫鱼最好不和什么一起吃？\"\n",
    "prompt_embedding = make_embeddings(client, [prompt])[0]\n",
    "\n",
    "context = search_similar_text(database_name, prompt_embedding)\n",
    "context = [text for text, embedding, similarity in context]\n",
    "chat_completion = get_response(\n",
    "    client, system_content=\"\", assistant_content=\"\".join(context), user_content=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'鲫鱼最好不要与芥菜、猪肝、猪肉、蒜、鸡肉、鲅鱼等一起食用。'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_590",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
